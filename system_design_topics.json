[
  {
    "id": 1,
    "title": "Scalability Patterns",
    "description": "Horizontal and vertical scaling strategies",
    "explanation": "Scalability enables systems to handle increased load. Horizontal scaling (scale-out) adds more servers, while vertical scaling (scale-up) adds resources to existing servers. Database sharding partitions data across multiple databases. Read replicas distribute read load. Partitioning strategies include range-based, hash-based, and geographic. Microservices architecture enables independent scaling of services.",
    "keyPoints": [
      "Horizontal scaling - add more servers, better fault tolerance, preferred for cloud",
      "Vertical scaling - add CPU/RAM to existing servers, simpler but limited",
      "Database sharding - partition data across databases by key (user ID, region)",
      "Read replicas - distribute read load, eventual consistency trade-off",
      "Partitioning strategies - range, hash, geographic, list-based partitioning",
      "Microservices - independently scalable services, polyglot persistence"
    ],
    "javaCode": "// Horizontal Scaling with Load Balancing\n@Configuration\npublic class LoadBalancedServiceConfig {\n\n  @Bean\n  @LoadBalanced\n  public RestTemplate restTemplate() {\n    return new RestTemplate();\n  }\n\n  @Bean\n  public Sampler defaultSampler() {\n    return Sampler.ALWAYS_SAMPLE;\n  }\n}\n\n@Service\npublic class UserService {\n\n  @Autowired\n  @LoadBalanced\n  private RestTemplate restTemplate;\n\n  public User getUser(Long id) {\n    // Load balancer automatically distributes across instances\n    return restTemplate.getForObject(\n      \"http://user-service/api/users/\" + id,\n      User.class\n    );\n  }\n}\n\n// Database Sharding Strategy\n@Service\npublic class ShardingService {\n\n  private final Map<Integer, DataSource> shards = new HashMap<>();\n\n  public ShardingService() {\n    // Initialize shards\n    shards.put(0, createDataSource(\"shard-0\"));\n    shards.put(1, createDataSource(\"shard-1\"));\n    shards.put(2, createDataSource(\"shard-2\"));\n    shards.put(3, createDataSource(\"shard-3\"));\n  }\n\n  public int getShardId(Long userId) {\n    // Hash-based sharding\n    return Math.abs(userId.hashCode() % shards.size());\n  }\n\n  public User getUserFromShard(Long userId) {\n    int shardId = getShardId(userId);\n    DataSource dataSource = shards.get(shardId);\n\n    JdbcTemplate jdbcTemplate = new JdbcTemplate(dataSource);\n    return jdbcTemplate.queryForObject(\n      \"SELECT * FROM users WHERE id = ?\",\n      userRowMapper,\n      userId\n    );\n  }\n\n  // Range-based sharding\n  public int getShardByRange(Long userId) {\n    if (userId < 1000000) return 0;\n    if (userId < 2000000) return 1;\n    if (userId < 3000000) return 2;\n    return 3;\n  }\n\n  // Geographic sharding\n  public int getShardByRegion(String region) {\n    return switch (region) {\n      case \"US-EAST\" -> 0;\n      case \"US-WEST\" -> 1;\n      case \"EU\" -> 2;\n      case \"ASIA\" -> 3;\n      default -> 0;\n    };\n  }\n}\n\n// Read Replica Configuration\n@Configuration\npublic class DataSourceConfig {\n\n  @Bean\n  @Primary\n  public DataSource primaryDataSource() {\n    return DataSourceBuilder.create()\n      .url(\"jdbc:mysql://primary-db:3306/mydb\")\n      .username(\"user\")\n      .password(\"password\")\n      .build();\n  }\n\n  @Bean\n  public DataSource replicaDataSource() {\n    return DataSourceBuilder.create()\n      .url(\"jdbc:mysql://replica-db:3306/mydb\")\n      .username(\"user\")\n      .password(\"password\")\n      .build();\n  }\n\n  @Bean\n  public DataSource routingDataSource() {\n    RoutingDataSource routingDataSource = new RoutingDataSource();\n\n    Map<Object, Object> dataSourceMap = new HashMap<>();\n    dataSourceMap.put(\"PRIMARY\", primaryDataSource());\n    dataSourceMap.put(\"REPLICA\", replicaDataSource());\n\n    routingDataSource.setTargetDataSources(dataSourceMap);\n    routingDataSource.setDefaultTargetDataSource(primaryDataSource());\n\n    return routingDataSource;\n  }\n}\n\npublic class RoutingDataSource extends AbstractRoutingDataSource {\n\n  @Override\n  protected Object determineCurrentLookupKey() {\n    return DataSourceContext.getDataSourceType();\n  }\n}\n\n@Service\npublic class UserQueryService {\n\n  @Transactional(readOnly = true)\n  public List<User> getAllUsers() {\n    // Route to read replica\n    DataSourceContext.setDataSourceType(\"REPLICA\");\n    return userRepository.findAll();\n  }\n\n  @Transactional\n  public User createUser(User user) {\n    // Route to primary\n    DataSourceContext.setDataSourceType(\"PRIMARY\");\n    return userRepository.save(user);\n  }\n}\n\n// Microservices Scaling\n/*\n# docker-compose.yml - Scale services independently\nversion: '3.8'\nservices:\n  user-service:\n    image: user-service:latest\n    deploy:\n      replicas: 5  # Scale to 5 instances\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n\n  order-service:\n    image: order-service:latest\n    deploy:\n      replicas: 3  # Scale based on load\n      resources:\n        limits:\n          cpus: '1.0'\n          memory: 1G\n\n  payment-service:\n    image: payment-service:latest\n    deploy:\n      replicas: 2  # Critical service\n      resources:\n        limits:\n          cpus: '2.0'\n          memory: 2G\n\n# Kubernetes auto-scaling\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: user-service-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: user-service\n  minReplicas: 3\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n*/\n\n// Vertical Scaling Configuration\n@Configuration\npublic class VerticalScalingConfig {\n\n  @Bean\n  public ThreadPoolTaskExecutor taskExecutor() {\n    ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();\n    executor.setCorePoolSize(20);      // Increased from 10\n    executor.setMaxPoolSize(100);      // Increased from 50\n    executor.setQueueCapacity(500);    // Increased from 200\n    executor.setThreadNamePrefix(\"async-\");\n    executor.initialize();\n    return executor;\n  }\n\n  @Bean\n  public HikariDataSource dataSource() {\n    HikariConfig config = new HikariConfig();\n    config.setJdbcUrl(\"jdbc:mysql://localhost:3306/mydb\");\n    config.setUsername(\"user\");\n    config.setPassword(\"password\");\n    config.setMaximumPoolSize(50);     // Increased pool size\n    config.setMinimumIdle(20);\n    config.setConnectionTimeout(30000);\n    return new HikariDataSource(config);\n  }\n}\n\n// Consistent Hashing for Distributed Caching\npublic class ConsistentHashing {\n\n  private final TreeMap<Long, String> ring = new TreeMap<>();\n  private final int virtualNodes = 150;\n\n  public void addNode(String node) {\n    for (int i = 0; i < virtualNodes; i++) {\n      long hash = hash(node + \":\" + i);\n      ring.put(hash, node);\n    }\n  }\n\n  public void removeNode(String node) {\n    for (int i = 0; i < virtualNodes; i++) {\n      long hash = hash(node + \":\" + i);\n      ring.remove(hash);\n    }\n  }\n\n  public String getNode(String key) {\n    if (ring.isEmpty()) return null;\n\n    long hash = hash(key);\n    Map.Entry<Long, String> entry = ring.ceilingEntry(hash);\n\n    if (entry == null) {\n      entry = ring.firstEntry();\n    }\n\n    return entry.getValue();\n  }\n\n  private long hash(String key) {\n    return key.hashCode() & 0xFFFFFFFFL;\n  }\n}",
    "difficulty": "Medium"
  },
  {
    "id": 2,
    "title": "High Availability",
    "description": "Ensuring system uptime and reliability",
    "explanation": "High Availability (HA) ensures systems remain operational during failures. Redundancy provides backup components. Active-Active configurations distribute load across all nodes. Active-Passive has standby nodes ready for failover. Health checks detect failures early. Circuit breakers prevent cascading failures using libraries like Resilience4j. Failover mechanisms automatically redirect traffic to healthy instances.",
    "keyPoints": [
      "Redundancy - eliminate single points of failure, duplicate critical components",
      "Active-Active - all nodes serve traffic, better resource utilization",
      "Active-Passive - standby nodes activate on failure, simpler failover",
      "Health checks - monitor service health, automated failure detection",
      "Circuit breakers (Resilience4j) - prevent cascade failures, fail fast",
      "Failover - automatic traffic redirection to healthy instances"
    ],
    "javaCode": "// Resilience4j Circuit Breaker\n@Configuration\npublic class CircuitBreakerConfig {\n\n  @Bean\n  public CircuitBreakerRegistry circuitBreakerRegistry() {\n    CircuitBreakerConfig config = CircuitBreakerConfig.custom()\n      .failureRateThreshold(50)                    // 50% failure rate\n      .waitDurationInOpenState(Duration.ofSeconds(30))\n      .slidingWindowSize(10)                       // Last 10 calls\n      .minimumNumberOfCalls(5)                     // Minimum calls before calculation\n      .permittedNumberOfCallsInHalfOpenState(3)\n      .automaticTransitionFromOpenToHalfOpenEnabled(true)\n      .build();\n\n    return CircuitBreakerRegistry.of(config);\n  }\n}\n\n@Service\npublic class PaymentService {\n\n  private final CircuitBreaker circuitBreaker;\n  private final RestTemplate restTemplate;\n\n  public PaymentService(CircuitBreakerRegistry registry) {\n    this.circuitBreaker = registry.circuitBreaker(\"payment-service\");\n    this.restTemplate = new RestTemplate();\n  }\n\n  public PaymentResponse processPayment(PaymentRequest request) {\n    // Wrap external call with circuit breaker\n    return circuitBreaker.executeSupplier(() -> {\n      try {\n        return restTemplate.postForObject(\n          \"http://payment-gateway/process\",\n          request,\n          PaymentResponse.class\n        );\n      } catch (Exception e) {\n        // Circuit breaker counts this as failure\n        throw new PaymentException(\"Payment failed\", e);\n      }\n    });\n  }\n\n  public PaymentResponse processPaymentWithFallback(PaymentRequest request) {\n    return circuitBreaker.executeSupplier(() ->\n      callPaymentGateway(request)\n    ).recover(throwable -> {\n      // Fallback: queue for retry\n      log.error(\"Payment failed, queuing for retry\", throwable);\n      queuePaymentForRetry(request);\n      return PaymentResponse.pending();\n    }).get();\n  }\n}\n\n// Health Check Implementation\n@RestController\npublic class HealthCheckController {\n\n  @Autowired\n  private DataSource dataSource;\n\n  @Autowired\n  private RedisTemplate<String, String> redisTemplate;\n\n  @GetMapping(\"/health\")\n  public ResponseEntity<HealthStatus> healthCheck() {\n    HealthStatus status = new HealthStatus();\n\n    // Check database connectivity\n    try (Connection conn = dataSource.getConnection()) {\n      status.setDatabase(\"UP\");\n    } catch (SQLException e) {\n      status.setDatabase(\"DOWN\");\n      status.setHealthy(false);\n    }\n\n    // Check Redis connectivity\n    try {\n      redisTemplate.opsForValue().get(\"health-check\");\n      status.setCache(\"UP\");\n    } catch (Exception e) {\n      status.setCache(\"DOWN\");\n      status.setHealthy(false);\n    }\n\n    // Check external dependencies\n    status.setPaymentGateway(checkPaymentGateway());\n\n    return status.isHealthy()\n      ? ResponseEntity.ok(status)\n      : ResponseEntity.status(503).body(status);\n  }\n\n  @GetMapping(\"/health/live\")\n  public ResponseEntity<String> liveness() {\n    // Simple liveness check - is app running?\n    return ResponseEntity.ok(\"ALIVE\");\n  }\n\n  @GetMapping(\"/health/ready\")\n  public ResponseEntity<String> readiness() {\n    // Readiness check - can app serve traffic?\n    if (isReadyToServeTraffic()) {\n      return ResponseEntity.ok(\"READY\");\n    }\n    return ResponseEntity.status(503).body(\"NOT_READY\");\n  }\n}\n\n// Active-Active High Availability\n/*\n# Kubernetes Deployment - Active-Active\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service\nspec:\n  replicas: 3  # Multiple active instances\n  selector:\n    matchLabels:\n      app: user-service\n  template:\n    metadata:\n      labels:\n        app: user-service\n    spec:\n      containers:\n      - name: user-service\n        image: user-service:latest\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health/live\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 5\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-service\nspec:\n  selector:\n    app: user-service\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: LoadBalancer\n*/\n\n// Retry with Exponential Backoff\n@Service\npublic class ResilientService {\n\n  @Retry(\n    name = \"userService\",\n    fallbackMethod = \"fallbackGetUser\"\n  )\n  @CircuitBreaker(\n    name = \"userService\",\n    fallbackMethod = \"fallbackGetUser\"\n  )\n  public User getUser(Long id) {\n    return restTemplate.getForObject(\n      \"http://user-service/api/users/\" + id,\n      User.class\n    );\n  }\n\n  private User fallbackGetUser(Long id, Exception e) {\n    log.warn(\"Fallback triggered for user: \" + id, e);\n    // Return cached data or default user\n    return userCache.getOrDefault(id, User.defaultUser());\n  }\n}\n\n// application.yml - Resilience4j Configuration\n/*\nresilience4j:\n  circuitbreaker:\n    instances:\n      userService:\n        registerHealthIndicator: true\n        slidingWindowSize: 10\n        minimumNumberOfCalls: 5\n        permittedNumberOfCallsInHalfOpenState: 3\n        automaticTransitionFromOpenToHalfOpenEnabled: true\n        waitDurationInOpenState: 30s\n        failureRateThreshold: 50\n        eventConsumerBufferSize: 10\n\n  retry:\n    instances:\n      userService:\n        maxAttempts: 3\n        waitDuration: 1s\n        enableExponentialBackoff: true\n        exponentialBackoffMultiplier: 2\n        retryExceptions:\n          - org.springframework.web.client.HttpServerErrorException\n          - java.net.SocketTimeoutException\n\n  timelimiter:\n    instances:\n      userService:\n        timeoutDuration: 3s\n        cancelRunningFuture: true\n*/\n\n// Failover with Service Discovery\n@Configuration\n@EnableDiscoveryClient\npublic class FailoverConfig {\n\n  @Bean\n  @LoadBalanced\n  public RestTemplate restTemplate() {\n    return new RestTemplate();\n  }\n}\n\n@Service\npublic class FailoverService {\n\n  @Autowired\n  private DiscoveryClient discoveryClient;\n\n  public User getUserWithFailover(Long id) {\n    List<ServiceInstance> instances =\n      discoveryClient.getInstances(\"user-service\");\n\n    for (ServiceInstance instance : instances) {\n      try {\n        String url = instance.getUri() + \"/api/users/\" + id;\n        return restTemplate.getForObject(url, User.class);\n      } catch (Exception e) {\n        log.warn(\"Instance {} failed, trying next\", instance.getUri());\n        continue;\n      }\n    }\n\n    throw new ServiceUnavailableException(\"All instances failed\");\n  }\n}\n\n// Database Failover\n@Configuration\npublic class DatabaseFailoverConfig {\n\n  @Bean\n  public DataSource dataSource() {\n    HikariConfig primary = new HikariConfig();\n    primary.setJdbcUrl(\"jdbc:mysql://primary:3306/db\");\n\n    HikariConfig replica = new HikariConfig();\n    replica.setJdbcUrl(\"jdbc:mysql://replica:3306/db\");\n\n    return new FailoverDataSource(\n      new HikariDataSource(primary),\n      new HikariDataSource(replica)\n    );\n  }\n}\n\npublic class FailoverDataSource extends AbstractRoutingDataSource {\n\n  private final DataSource primary;\n  private final DataSource replica;\n  private volatile boolean primaryHealthy = true;\n\n  public FailoverDataSource(DataSource primary, DataSource replica) {\n    this.primary = primary;\n    this.replica = replica;\n\n    Map<Object, Object> dataSourceMap = new HashMap<>();\n    dataSourceMap.put(\"PRIMARY\", primary);\n    dataSourceMap.put(\"REPLICA\", replica);\n\n    setTargetDataSources(dataSourceMap);\n    setDefaultTargetDataSource(primary);\n\n    // Start health check\n    scheduleHealthCheck();\n  }\n\n  @Override\n  protected Object determineCurrentLookupKey() {\n    return primaryHealthy ? \"PRIMARY\" : \"REPLICA\";\n  }\n\n  private void scheduleHealthCheck() {\n    ScheduledExecutorService executor = Executors.newScheduledThreadPool(1);\n    executor.scheduleAtFixedRate(() -> {\n      try (Connection conn = primary.getConnection()) {\n        conn.createStatement().execute(\"SELECT 1\");\n        primaryHealthy = true;\n      } catch (SQLException e) {\n        log.error(\"Primary database down, failing over to replica\");\n        primaryHealthy = false;\n      }\n    }, 0, 10, TimeUnit.SECONDS);\n  }\n}",
    "difficulty": "Medium"
  },
  {
    "id": 3,
    "title": "Caching Strategies",
    "description": "Cache patterns for performance",
    "explanation": "Caching stores frequently accessed data in fast storage to reduce latency and database load. Cache-aside (lazy loading) loads data on cache miss. Write-through updates cache and database synchronously. Write-behind queues writes for async persistence. Redis and Memcached provide distributed caching. CDN caches static content at edge locations. Cache invalidation strategies include TTL, event-based, and manual invalidation.",
    "keyPoints": [
      "Cache-aside (lazy loading) - read from cache, load on miss, app manages cache",
      "Write-through - write to cache and database synchronously, ensures consistency",
      "Write-behind (write-back) - write to cache immediately, persist async to database",
      "Redis/Memcached - distributed in-memory caches, Redis supports data structures",
      "CDN - caches static assets at edge locations, reduces latency globally",
      "Cache invalidation - TTL expiration, event-based, manual purge strategies"
    ],
    "javaCode": "// Cache-Aside Pattern (Lazy Loading)\n@Service\npublic class UserService {\n\n  @Autowired\n  private UserRepository userRepository;\n\n  @Autowired\n  private RedisTemplate<String, User> redisTemplate;\n\n  private static final String CACHE_KEY_PREFIX = \"user:\";\n  private static final long CACHE_TTL = 3600; // 1 hour\n\n  public User getUserById(Long id) {\n    String cacheKey = CACHE_KEY_PREFIX + id;\n\n    // Try cache first\n    User cachedUser = redisTemplate.opsForValue().get(cacheKey);\n    if (cachedUser != null) {\n      log.debug(\"Cache hit for user: {}\", id);\n      return cachedUser;\n    }\n\n    // Cache miss - load from database\n    log.debug(\"Cache miss for user: {}\", id);\n    User user = userRepository.findById(id)\n      .orElseThrow(() -> new UserNotFoundException(id));\n\n    // Update cache\n    redisTemplate.opsForValue().set(cacheKey, user, CACHE_TTL, TimeUnit.SECONDS);\n\n    return user;\n  }\n\n  public void deleteUser(Long id) {\n    userRepository.deleteById(id);\n\n    // Invalidate cache\n    String cacheKey = CACHE_KEY_PREFIX + id;\n    redisTemplate.delete(cacheKey);\n  }\n}\n\n// Write-Through Pattern\n@Service\npublic class WriteThrough Cache Service {\n\n  @Autowired\n  private ProductRepository productRepository;\n\n  @Autowired\n  private RedisTemplate<String, Product> redisTemplate;\n\n  @Transactional\n  public Product updateProduct(Product product) {\n    // Write to database\n    Product saved = productRepository.save(product);\n\n    // Write to cache (synchronously)\n    String cacheKey = \"product:\" + product.getId();\n    redisTemplate.opsForValue().set(cacheKey, saved, 1, TimeUnit.HOURS);\n\n    return saved;\n  }\n\n  public Product getProduct(Long id) {\n    String cacheKey = \"product:\" + id;\n\n    // Read from cache\n    Product cached = redisTemplate.opsForValue().get(cacheKey);\n    if (cached != null) {\n      return cached;\n    }\n\n    // Load from database and cache\n    Product product = productRepository.findById(id).orElseThrow();\n    redisTemplate.opsForValue().set(cacheKey, product, 1, TimeUnit.HOURS);\n\n    return product;\n  }\n}\n\n// Write-Behind (Write-Back) Pattern\n@Service\npublic class WriteBehindCacheService {\n\n  @Autowired\n  private RedisTemplate<String, Order> redisTemplate;\n\n  @Autowired\n  private OrderRepository orderRepository;\n\n  private final BlockingQueue<Order> writeQueue = new LinkedBlockingQueue<>();\n\n  @PostConstruct\n  public void startAsyncWriter() {\n    ExecutorService executor = Executors.newSingleThreadExecutor();\n    executor.submit(() -> {\n      while (true) {\n        try {\n          Order order = writeQueue.take();\n          orderRepository.save(order);\n          log.debug(\"Persisted order: {}\", order.getId());\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          break;\n        }\n      }\n    });\n  }\n\n  public Order createOrder(Order order) {\n    order.setCreatedAt(LocalDateTime.now());\n\n    // Write to cache immediately\n    String cacheKey = \"order:\" + order.getId();\n    redisTemplate.opsForValue().set(cacheKey, order, 30, TimeUnit.MINUTES);\n\n    // Queue for async database write\n    writeQueue.offer(order);\n\n    return order;\n  }\n}\n\n// Spring Cache Abstraction\n@Configuration\n@EnableCaching\npublic class CacheConfig {\n\n  @Bean\n  public CacheManager cacheManager(RedisConnectionFactory factory) {\n    RedisCacheConfiguration config = RedisCacheConfiguration.defaultCacheConfig()\n      .entryTtl(Duration.ofHours(1))\n      .serializeKeysWith(\n        RedisSerializationContext.SerializationPair.fromSerializer(\n          new StringRedisSerializer()\n        )\n      )\n      .serializeValuesWith(\n        RedisSerializationContext.SerializationPair.fromSerializer(\n          new GenericJackson2JsonRedisSerializer()\n        )\n      );\n\n    return RedisCacheManager.builder(factory)\n      .cacheDefaults(config)\n      .build();\n  }\n}\n\n@Service\npublic class CachedUserService {\n\n  @Cacheable(value = \"users\", key = \"#id\")\n  public User getUserById(Long id) {\n    log.info(\"Fetching user from database: {}\", id);\n    return userRepository.findById(id).orElseThrow();\n  }\n\n  @CachePut(value = \"users\", key = \"#user.id\")\n  public User updateUser(User user) {\n    log.info(\"Updating user: {}\", user.getId());\n    return userRepository.save(user);\n  }\n\n  @CacheEvict(value = \"users\", key = \"#id\")\n  public void deleteUser(Long id) {\n    log.info(\"Deleting user: {}\", id);\n    userRepository.deleteById(id);\n  }\n\n  @CacheEvict(value = \"users\", allEntries = true)\n  public void clearAllUsers() {\n    log.info(\"Clearing all users from cache\");\n  }\n}\n\n// Cache Invalidation Strategies\n@Service\npublic class CacheInvalidationService {\n\n  @Autowired\n  private RedisTemplate<String, Object> redisTemplate;\n\n  // TTL-based invalidation (automatic)\n  public void setWithTtl(String key, Object value, long ttlSeconds) {\n    redisTemplate.opsForValue().set(key, value, ttlSeconds, TimeUnit.SECONDS);\n  }\n\n  // Event-based invalidation\n  @EventListener\n  public void onUserUpdated(UserUpdatedEvent event) {\n    String cacheKey = \"user:\" + event.getUserId();\n    redisTemplate.delete(cacheKey);\n    log.info(\"Invalidated cache for user: {}\", event.getUserId());\n  }\n\n  // Pattern-based invalidation\n  public void invalidateByPattern(String pattern) {\n    Set<String> keys = redisTemplate.keys(pattern);\n    if (keys != null && !keys.isEmpty()) {\n      redisTemplate.delete(keys);\n      log.info(\"Invalidated {} keys matching pattern: {}\", keys.size(), pattern);\n    }\n  }\n\n  // Bulk invalidation\n  public void invalidateMultiple(List<String> keys) {\n    redisTemplate.delete(keys);\n  }\n}\n\n// Multi-Level Caching\n@Service\npublic class MultiLevelCacheService {\n\n  // L1: Local cache (Caffeine)\n  private final Cache<String, User> localCache = Caffeine.newBuilder()\n    .maximumSize(1000)\n    .expireAfterWrite(5, TimeUnit.MINUTES)\n    .build();\n\n  // L2: Distributed cache (Redis)\n  @Autowired\n  private RedisTemplate<String, User> redisTemplate;\n\n  @Autowired\n  private UserRepository userRepository;\n\n  public User getUser(Long id) {\n    String key = \"user:\" + id;\n\n    // Check L1 cache\n    User user = localCache.getIfPresent(key);\n    if (user != null) {\n      log.debug(\"L1 cache hit: {}\", id);\n      return user;\n    }\n\n    // Check L2 cache\n    user = redisTemplate.opsForValue().get(key);\n    if (user != null) {\n      log.debug(\"L2 cache hit: {}\", id);\n      localCache.put(key, user);\n      return user;\n    }\n\n    // Load from database\n    log.debug(\"Cache miss, loading from DB: {}\", id);\n    user = userRepository.findById(id).orElseThrow();\n\n    // Populate caches\n    localCache.put(key, user);\n    redisTemplate.opsForValue().set(key, user, 1, TimeUnit.HOURS);\n\n    return user;\n  }\n}\n\n// CDN Configuration (Nginx)\n/*\n# nginx.conf - CDN/Reverse Proxy Cache\nhttp {\n  proxy_cache_path /var/cache/nginx levels=1:2\n    keys_zone=static_cache:10m max_size=1g\n    inactive=60m use_temp_path=off;\n\n  server {\n    listen 80;\n\n    location /static/ {\n      proxy_pass http://backend;\n      proxy_cache static_cache;\n      proxy_cache_valid 200 60m;\n      proxy_cache_valid 404 1m;\n      proxy_cache_use_stale error timeout updating;\n      add_header X-Cache-Status $upstream_cache_status;\n    }\n\n    location /api/ {\n      proxy_pass http://backend;\n      proxy_cache api_cache;\n      proxy_cache_valid 200 5m;\n      proxy_cache_key \"$request_uri|$http_accept|$http_accept_encoding\";\n      proxy_cache_bypass $http_cache_control;\n    }\n  }\n}\n*/\n\n// Cache Warming\n@Component\npublic class CacheWarmer {\n\n  @Autowired\n  private UserService userService;\n\n  @Autowired\n  private ProductService productService;\n\n  @EventListener(ApplicationReadyEvent.class)\n  public void warmCache() {\n    log.info(\"Starting cache warming...\");\n\n    // Warm frequently accessed users\n    List<Long> popularUserIds = Arrays.asList(1L, 2L, 3L, 4L, 5L);\n    popularUserIds.forEach(userService::getUserById);\n\n    // Warm popular products\n    productService.getTopProducts(100)\n      .forEach(product -> productService.getProduct(product.getId()));\n\n    log.info(\"Cache warming completed\");\n  }\n}",
    "difficulty": "Medium"
  },
  {
    "id": 4,
    "title": "N-Tier Architecture",
    "description": "Layered application architecture",
    "explanation": "N-Tier architecture separates applications into logical layers with distinct responsibilities. The Presentation layer handles UI and user interaction. The Business layer contains domain logic and business rules. The Data layer manages persistence and database operations. Separation of concerns improves maintainability, testability, and allows independent scaling. Each layer should only communicate with adjacent layers.",
    "keyPoints": [
      "Presentation layer - UI, controllers, view models, user interaction",
      "Business layer - domain logic, business rules, validation, workflows",
      "Data layer - repositories, DAO, database access, ORM mapping",
      "Separation of concerns - each layer has single responsibility",
      "Layered architecture - strict dependency rules, top-down communication",
      "Cross-cutting concerns - logging, security, transaction management"
    ],
    "javaCode": "// Presentation Layer - Controllers\n@RestController\n@RequestMapping(\"/api/orders\")\npublic class OrderController {\n\n  @Autowired\n  private OrderService orderService;\n\n  @PostMapping\n  public ResponseEntity<OrderDto> createOrder(@Valid @RequestBody CreateOrderRequest request) {\n    Order order = orderService.createOrder(\n      request.getUserId(),\n      request.getItems()\n    );\n\n    OrderDto dto = OrderMapper.toDto(order);\n    return ResponseEntity.status(HttpStatus.CREATED).body(dto);\n  }\n\n  @GetMapping(\"/{id}\")\n  public ResponseEntity<OrderDto> getOrder(@PathVariable Long id) {\n    Order order = orderService.getOrderById(id);\n    return ResponseEntity.ok(OrderMapper.toDto(order));\n  }\n\n  @GetMapping(\"/user/{userId}\")\n  public ResponseEntity<List<OrderDto>> getUserOrders(@PathVariable Long userId) {\n    List<Order> orders = orderService.getOrdersByUserId(userId);\n    List<OrderDto> dtos = orders.stream()\n      .map(OrderMapper::toDto)\n      .collect(Collectors.toList());\n    return ResponseEntity.ok(dtos);\n  }\n}\n\n// DTO (Data Transfer Object)\npublic class OrderDto {\n  private Long id;\n  private Long userId;\n  private BigDecimal totalAmount;\n  private String status;\n  private LocalDateTime createdAt;\n  private List<OrderItemDto> items;\n\n  // Getters and setters\n}\n\n// Business Layer - Service\n@Service\n@Transactional\npublic class OrderService {\n\n  @Autowired\n  private OrderRepository orderRepository;\n\n  @Autowired\n  private UserRepository userRepository;\n\n  @Autowired\n  private ProductRepository productRepository;\n\n  @Autowired\n  private InventoryService inventoryService;\n\n  @Autowired\n  private PaymentService paymentService;\n\n  public Order createOrder(Long userId, List<OrderItemRequest> items) {\n    // Business logic validation\n    User user = userRepository.findById(userId)\n      .orElseThrow(() -> new UserNotFoundException(userId));\n\n    if (!user.isActive()) {\n      throw new BusinessException(\"User account is not active\");\n    }\n\n    // Calculate total and validate inventory\n    BigDecimal total = BigDecimal.ZERO;\n    List<OrderItem> orderItems = new ArrayList<>();\n\n    for (OrderItemRequest item : items) {\n      Product product = productRepository.findById(item.getProductId())\n        .orElseThrow(() -> new ProductNotFoundException(item.getProductId()));\n\n      // Check inventory\n      if (!inventoryService.checkAvailability(product.getId(), item.getQuantity())) {\n        throw new InsufficientInventoryException(product.getId());\n      }\n\n      BigDecimal itemTotal = product.getPrice()\n        .multiply(BigDecimal.valueOf(item.getQuantity()));\n      total = total.add(itemTotal);\n\n      OrderItem orderItem = new OrderItem();\n      orderItem.setProduct(product);\n      orderItem.setQuantity(item.getQuantity());\n      orderItem.setPrice(product.getPrice());\n      orderItems.add(orderItem);\n    }\n\n    // Apply business rules\n    if (total.compareTo(BigDecimal.valueOf(1000)) > 0) {\n      // Apply discount for orders over $1000\n      total = total.multiply(BigDecimal.valueOf(0.95));\n    }\n\n    // Create order\n    Order order = new Order();\n    order.setUser(user);\n    order.setItems(orderItems);\n    order.setTotalAmount(total);\n    order.setStatus(OrderStatus.PENDING);\n    order.setCreatedAt(LocalDateTime.now());\n\n    // Save order\n    Order savedOrder = orderRepository.save(order);\n\n    // Reserve inventory\n    orderItems.forEach(item ->\n      inventoryService.reserve(item.getProduct().getId(), item.getQuantity())\n    );\n\n    // Process payment asynchronously\n    paymentService.processPaymentAsync(savedOrder);\n\n    return savedOrder;\n  }\n\n  @Transactional(readOnly = true)\n  public Order getOrderById(Long id) {\n    return orderRepository.findById(id)\n      .orElseThrow(() -> new OrderNotFoundException(id));\n  }\n\n  @Transactional(readOnly = true)\n  public List<Order> getOrdersByUserId(Long userId) {\n    return orderRepository.findByUserId(userId);\n  }\n\n  public void cancelOrder(Long orderId) {\n    Order order = getOrderById(orderId);\n\n    // Business rule: can only cancel pending orders\n    if (order.getStatus() != OrderStatus.PENDING) {\n      throw new BusinessException(\"Cannot cancel order in status: \" + order.getStatus());\n    }\n\n    // Release inventory\n    order.getItems().forEach(item ->\n      inventoryService.release(item.getProduct().getId(), item.getQuantity())\n    );\n\n    // Update status\n    order.setStatus(OrderStatus.CANCELLED);\n    orderRepository.save(order);\n  }\n}\n\n// Data Layer - Repository\n@Repository\npublic interface OrderRepository extends JpaRepository<Order, Long> {\n\n  List<Order> findByUserId(Long userId);\n\n  List<Order> findByStatus(OrderStatus status);\n\n  @Query(\"SELECT o FROM Order o WHERE o.createdAt BETWEEN :start AND :end\")\n  List<Order> findByDateRange(\n    @Param(\"start\") LocalDateTime start,\n    @Param(\"end\") LocalDateTime end\n  );\n\n  @Query(\"SELECT o FROM Order o JOIN FETCH o.items WHERE o.id = :id\")\n  Optional<Order> findByIdWithItems(@Param(\"id\") Long id);\n}\n\n// Domain Model\n@Entity\n@Table(name = \"orders\")\npublic class Order {\n\n  @Id\n  @GeneratedValue(strategy = GenerationType.IDENTITY)\n  private Long id;\n\n  @ManyToOne\n  @JoinColumn(name = \"user_id\", nullable = false)\n  private User user;\n\n  @OneToMany(cascade = CascadeType.ALL, orphanRemoval = true)\n  @JoinColumn(name = \"order_id\")\n  private List<OrderItem> items = new ArrayList<>();\n\n  @Column(nullable = false)\n  private BigDecimal totalAmount;\n\n  @Enumerated(EnumType.STRING)\n  private OrderStatus status;\n\n  private LocalDateTime createdAt;\n\n  // Getters and setters\n}\n\n@Entity\n@Table(name = \"order_items\")\npublic class OrderItem {\n\n  @Id\n  @GeneratedValue(strategy = GenerationType.IDENTITY)\n  private Long id;\n\n  @ManyToOne\n  @JoinColumn(name = \"product_id\")\n  private Product product;\n\n  private Integer quantity;\n  private BigDecimal price;\n\n  // Getters and setters\n}\n\n// Cross-Cutting Concerns - Aspect\n@Aspect\n@Component\npublic class LoggingAspect {\n\n  @Around(\"@annotation(org.springframework.web.bind.annotation.RequestMapping)\")\n  public Object logControllerAccess(ProceedingJoinPoint joinPoint) throws Throwable {\n    String method = joinPoint.getSignature().getName();\n    log.info(\"Controller method called: {}\", method);\n\n    long start = System.currentTimeMillis();\n    Object result = joinPoint.proceed();\n    long duration = System.currentTimeMillis() - start;\n\n    log.info(\"Controller method {} completed in {}ms\", method, duration);\n    return result;\n  }\n\n  @Around(\"@within(org.springframework.stereotype.Service)\")\n  public Object logServiceAccess(ProceedingJoinPoint joinPoint) throws Throwable {\n    String method = joinPoint.getSignature().getName();\n    Object[] args = joinPoint.getArgs();\n\n    log.debug(\"Service method called: {} with args: {}\", method, args);\n\n    try {\n      Object result = joinPoint.proceed();\n      log.debug(\"Service method {} returned: {}\", method, result);\n      return result;\n    } catch (Exception e) {\n      log.error(\"Service method {} threw exception\", method, e);\n      throw e;\n    }\n  }\n}\n\n// Transaction Management\n@Configuration\n@EnableTransactionManagement\npublic class TransactionConfig {\n\n  @Bean\n  public PlatformTransactionManager transactionManager(EntityManagerFactory emf) {\n    return new JpaTransactionManager(emf);\n  }\n}\n\n// Layered Architecture Diagram\n/*\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Presentation Layer             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502Controllers\u2502  \u2502DTO/ViewModels\u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        Business Layer               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502Services \u2502  \u2502Domain Entities \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Data Layer                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502Repositories\u2502  \u2502JPA/Hibernate \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          Database                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n*/",
    "difficulty": "Medium"
  },
  {
    "id": 5,
    "title": "Load Balancing",
    "description": "Distributing traffic across servers",
    "explanation": "Load balancing distributes incoming traffic across multiple servers to optimize resource usage, maximize throughput, and ensure high availability. Round-robin rotates requests across servers. Least connections sends traffic to servers with fewest active connections. Sticky sessions route user requests to the same server. Layer 4 (L4) operates at transport layer, Layer 7 (L7) at application layer. Health checks ensure traffic only goes to healthy servers.",
    "keyPoints": [
      "Round-robin - sequential distribution, simple and fair",
      "Least connections - routes to server with fewest active connections",
      "Sticky sessions (session affinity) - same user routes to same server",
      "L4 vs L7 balancing - L4 (TCP/UDP), L7 (HTTP/HTTPS with content routing)",
      "Health checks - active/passive monitoring, remove unhealthy servers",
      "Nginx/HAProxy - popular load balancers, reverse proxy capabilities"
    ],
    "javaCode": "// Spring Cloud LoadBalancer\n@Configuration\npublic class LoadBalancerConfig {\n\n  @Bean\n  @LoadBalanced\n  public RestTemplate restTemplate() {\n    return new RestTemplate();\n  }\n\n  @Bean\n  public ServiceInstanceListSupplier customServiceInstanceListSupplier(\n      ConfigurableApplicationContext context) {\n    return ServiceInstanceListSupplier.builder()\n      .withDiscoveryClient()\n      .withHealthChecks()\n      .withCaching()\n      .build(context);\n  }\n}\n\n@Service\npublic class LoadBalancedService {\n\n  @Autowired\n  @LoadBalanced\n  private RestTemplate restTemplate;\n\n  public User getUser(Long id) {\n    // Load balancer automatically distributes across instances\n    return restTemplate.getForObject(\n      \"http://user-service/api/users/\" + id,\n      User.class\n    );\n  }\n}\n\n// Custom Load Balancer\npublic class CustomLoadBalancer {\n\n  private final List<Server> servers = new CopyOnWriteArrayList<>();\n  private final AtomicInteger currentIndex = new AtomicInteger(0);\n\n  public void addServer(Server server) {\n    servers.add(server);\n  }\n\n  public void removeServer(Server server) {\n    servers.remove(server);\n  }\n\n  // Round-robin algorithm\n  public Server roundRobin() {\n    if (servers.isEmpty()) {\n      throw new NoAvailableServersException();\n    }\n\n    int index = currentIndex.getAndIncrement() % servers.size();\n    return servers.get(index);\n  }\n\n  // Least connections algorithm\n  public Server leastConnections() {\n    return servers.stream()\n      .filter(Server::isHealthy)\n      .min(Comparator.comparingInt(Server::getActiveConnections))\n      .orElseThrow(NoAvailableServersException::new);\n  }\n\n  // Weighted round-robin\n  public Server weightedRoundRobin() {\n    int totalWeight = servers.stream()\n      .mapToInt(Server::getWeight)\n      .sum();\n\n    int random = ThreadLocalRandom.current().nextInt(totalWeight);\n    int weightSum = 0;\n\n    for (Server server : servers) {\n      weightSum += server.getWeight();\n      if (random < weightSum) {\n        return server;\n      }\n    }\n\n    return servers.get(0);\n  }\n\n  // IP Hash (consistent hashing)\n  public Server ipHash(String clientIp) {\n    int hash = clientIp.hashCode();\n    int index = Math.abs(hash % servers.size());\n    return servers.get(index);\n  }\n}\n\npublic class Server {\n  private final String host;\n  private final int port;\n  private final AtomicInteger activeConnections = new AtomicInteger(0);\n  private volatile boolean healthy = true;\n  private final int weight;\n\n  public void incrementConnections() {\n    activeConnections.incrementAndGet();\n  }\n\n  public void decrementConnections() {\n    activeConnections.decrementAndGet();\n  }\n\n  // Getters\n}\n\n// Nginx Load Balancer Configuration\n/*\n# nginx.conf\nupstream backend {\n  # Round-robin (default)\n  server backend1.example.com:8080;\n  server backend2.example.com:8080;\n  server backend3.example.com:8080;\n}\n\nupstream weighted_backend {\n  # Weighted round-robin\n  server backend1.example.com:8080 weight=3;\n  server backend2.example.com:8080 weight=2;\n  server backend3.example.com:8080 weight=1;\n}\n\nupstream least_conn_backend {\n  # Least connections\n  least_conn;\n  server backend1.example.com:8080;\n  server backend2.example.com:8080;\n}\n\nupstream ip_hash_backend {\n  # IP Hash (sticky sessions)\n  ip_hash;\n  server backend1.example.com:8080;\n  server backend2.example.com:8080;\n}\n\n# Health checks\nupstream healthy_backend {\n  server backend1.example.com:8080 max_fails=3 fail_timeout=30s;\n  server backend2.example.com:8080 max_fails=3 fail_timeout=30s;\n  server backend3.example.com:8080 backup;  # Backup server\n}\n\nserver {\n  listen 80;\n\n  location / {\n    proxy_pass http://backend;\n    proxy_set_header Host $host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n    # Connection settings\n    proxy_connect_timeout 5s;\n    proxy_send_timeout 10s;\n    proxy_read_timeout 10s;\n\n    # Retry on error\n    proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;\n  }\n\n  # Layer 7 routing based on URI\n  location /api/ {\n    proxy_pass http://api_backend;\n  }\n\n  location /admin/ {\n    proxy_pass http://admin_backend;\n  }\n}\n*/\n\n// Health Check Service\n@Service\npublic class HealthCheckService {\n\n  private final Map<String, ServerHealth> serverHealth = new ConcurrentHashMap<>();\n  private final ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);\n\n  @PostConstruct\n  public void startHealthChecks() {\n    scheduler.scheduleAtFixedRate(\n      this::performHealthChecks,\n      0,\n      10,\n      TimeUnit.SECONDS\n    );\n  }\n\n  private void performHealthChecks() {\n    servers.forEach(server -> {\n      boolean healthy = checkServerHealth(server);\n      serverHealth.put(server.getUrl(), new ServerHealth(healthy, LocalDateTime.now()));\n\n      if (!healthy) {\n        log.warn(\"Server unhealthy: {}\", server.getUrl());\n      }\n    });\n  }\n\n  private boolean checkServerHealth(Server server) {\n    try {\n      RestTemplate restTemplate = new RestTemplate();\n      ResponseEntity<String> response = restTemplate.getForEntity(\n        server.getUrl() + \"/health\",\n        String.class\n      );\n      return response.getStatusCode() == HttpStatus.OK;\n    } catch (Exception e) {\n      log.error(\"Health check failed for {}\", server.getUrl(), e);\n      return false;\n    }\n  }\n\n  public boolean isHealthy(String serverUrl) {\n    ServerHealth health = serverHealth.get(serverUrl);\n    return health != null && health.isHealthy();\n  }\n}\n\n// Sticky Session Implementation\n@Component\npublic class StickySessionLoadBalancer {\n\n  private final Map<String, Server> sessionToServer = new ConcurrentHashMap<>();\n  private final LoadBalancer loadBalancer;\n\n  public Server getServer(String sessionId) {\n    // Check if session already mapped\n    if (sessionId != null && sessionToServer.containsKey(sessionId)) {\n      Server server = sessionToServer.get(sessionId);\n      if (server.isHealthy()) {\n        return server;\n      }\n    }\n\n    // Assign new server\n    Server server = loadBalancer.selectServer();\n    if (sessionId != null) {\n      sessionToServer.put(sessionId, server);\n    }\n\n    return server;\n  }\n\n  public void removeSession(String sessionId) {\n    sessionToServer.remove(sessionId);\n  }\n}\n\n// Ribbon Load Balancer (Legacy Spring Cloud)\n@Configuration\npublic class RibbonConfig {\n\n  @Bean\n  public IRule ribbonRule() {\n    // Available rules:\n    // - RoundRobinRule\n    // - RandomRule\n    // - WeightedResponseTimeRule\n    // - BestAvailableRule\n    // - AvailabilityFilteringRule\n    return new WeightedResponseTimeRule();\n  }\n}\n\n// Kubernetes Service Load Balancing\n/*\n# service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-service\nspec:\n  selector:\n    app: user-service\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: LoadBalancer\n  sessionAffinity: ClientIP  # Sticky sessions\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 10800\n\n---\n# Ingress with load balancing\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: app-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/load-balance: \"least_conn\"\n    nginx.ingress.kubernetes.io/upstream-hash-by: \"$remote_addr\"\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: user-service\n            port:\n              number: 80\n*/",
    "difficulty": "Medium"
  },
  {
    "id": 6,
    "title": "CAP Theorem",
    "description": "Consistency, Availability, Partition Tolerance",
    "explanation": "CAP theorem states that distributed systems can only guarantee two of three properties: Consistency (all nodes see same data), Availability (system responds to requests), Partition Tolerance (system works despite network failures). During network partitions, choose CP (consistent but unavailable) or AP (available but inconsistent). Eventual consistency accepts temporary inconsistencies. BASE (Basically Available, Soft state, Eventual consistency) is an alternative to ACID for distributed systems.",
    "keyPoints": [
      "Consistency - all nodes see the same data at the same time",
      "Availability - every request receives a response (success/failure)",
      "Partition Tolerance - system continues despite network failures",
      "CP systems - sacrifice availability for consistency (banks, inventory)",
      "AP systems - sacrifice consistency for availability (social media, caching)",
      "Eventual consistency - accepts temporary inconsistency, converges over time",
      "BASE - Basically Available, Soft state, Eventual consistency model"
    ],
    "javaCode": "// CP System - Strong Consistency (Banking)\n@Service\n@Transactional\npublic class BankingService {\n\n  @Autowired\n  private AccountRepository accountRepository;\n\n  // Strong consistency - use distributed locks\n  public void transfer(Long fromAccountId, Long toAccountId, BigDecimal amount) {\n    // Acquire locks in order to prevent deadlock\n    Long firstLock = Math.min(fromAccountId, toAccountId);\n    Long secondLock = Math.max(fromAccountId, toAccountId);\n\n    synchronized (getLock(firstLock)) {\n      synchronized (getLock(secondLock)) {\n        Account fromAccount = accountRepository.findById(fromAccountId)\n          .orElseThrow();\n        Account toAccount = accountRepository.findById(toAccountId)\n          .orElseThrow();\n\n        if (fromAccount.getBalance().compareTo(amount) < 0) {\n          throw new InsufficientFundsException();\n        }\n\n        // Both updates happen atomically\n        fromAccount.setBalance(fromAccount.getBalance().subtract(amount));\n        toAccount.setBalance(toAccount.getBalance().add(amount));\n\n        accountRepository.save(fromAccount);\n        accountRepository.save(toAccount);\n      }\n    }\n  }\n\n  private Object getLock(Long id) {\n    return (\"lock_\" + id).intern();\n  }\n}\n\n// AP System - Eventual Consistency (Social Media)\n@Service\npublic class SocialMediaService {\n\n  @Autowired\n  private PostRepository postRepository;\n\n  @Autowired\n  private KafkaTemplate<String, PostEvent> kafkaTemplate;\n\n  public Post createPost(Post post) {\n    // Write to local database (available)\n    Post saved = postRepository.save(post);\n\n    // Asynchronously propagate to other regions\n    PostEvent event = new PostEvent(saved.getId(), saved.getContent());\n    kafkaTemplate.send(\"post-events\", event);\n\n    // Return immediately without waiting for propagation\n    return saved;\n  }\n\n  @KafkaListener(topics = \"post-events\")\n  public void handlePostEvent(PostEvent event) {\n    // Eventually consistent - update local copy\n    Post post = new Post();\n    post.setId(event.getPostId());\n    post.setContent(event.getContent());\n    postRepository.save(post);\n  }\n}\n\n// Quorum-based Consistency\n@Service\npublic class QuorumService {\n\n  private final List<DataNode> nodes;\n  private final int writeQuorum;\n  private final int readQuorum;\n\n  public QuorumService(List<DataNode> nodes) {\n    this.nodes = nodes;\n    this.writeQuorum = (nodes.size() / 2) + 1;  // Majority\n    this.readQuorum = (nodes.size() / 2) + 1;\n  }\n\n  public void write(String key, String value) {\n    int successCount = 0;\n    List<CompletableFuture<Boolean>> futures = new ArrayList<>();\n\n    for (DataNode node : nodes) {\n      CompletableFuture<Boolean> future = CompletableFuture.supplyAsync(() ->\n        node.write(key, value)\n      );\n      futures.add(future);\n    }\n\n    // Wait for write quorum\n    for (CompletableFuture<Boolean> future : futures) {\n      try {\n        if (future.get(5, TimeUnit.SECONDS)) {\n          successCount++;\n          if (successCount >= writeQuorum) {\n            return;  // Success\n          }\n        }\n      } catch (Exception e) {\n        log.warn(\"Write failed on node\", e);\n      }\n    }\n\n    throw new QuorumNotReachedException(\"Failed to reach write quorum\");\n  }\n\n  public String read(String key) {\n    Map<String, Integer> versionCounts = new HashMap<>();\n    int responseCount = 0;\n\n    for (DataNode node : nodes) {\n      try {\n        VersionedValue value = node.read(key);\n        String data = value.getData();\n        versionCounts.merge(data, 1, Integer::sum);\n        responseCount++;\n\n        if (responseCount >= readQuorum) {\n          break;\n        }\n      } catch (Exception e) {\n        log.warn(\"Read failed on node\", e);\n      }\n    }\n\n    if (responseCount < readQuorum) {\n      throw new QuorumNotReachedException(\"Failed to reach read quorum\");\n    }\n\n    // Return most common version\n    return versionCounts.entrySet().stream()\n      .max(Map.Entry.comparingByValue())\n      .map(Map.Entry::getKey)\n      .orElseThrow();\n  }\n}\n\n// Eventual Consistency with Conflict Resolution\n@Service\npublic class EventuallyConsistentService {\n\n  public void mergeConflictingVersions(\n      List<VersionedData> versions,\n      String key) {\n\n    // Last-Write-Wins (LWW)\n    VersionedData latest = versions.stream()\n      .max(Comparator.comparing(VersionedData::getTimestamp))\n      .orElseThrow();\n\n    saveResolved(key, latest);\n  }\n\n  // Vector clock for conflict detection\n  public VersionedData readWithVectorClock(String key) {\n    List<VersionedData> versions = readFromAllReplicas(key);\n\n    // Check for conflicts using vector clocks\n    if (hasConflict(versions)) {\n      // Merge conflicts\n      return mergeConflicts(versions);\n    }\n\n    return versions.get(0);\n  }\n\n  private boolean hasConflict(List<VersionedData> versions) {\n    if (versions.size() <= 1) return false;\n\n    VersionedData first = versions.get(0);\n    return versions.stream()\n      .anyMatch(v -> !v.getVectorClock().equals(first.getVectorClock()));\n  }\n}\n\n// CRDT (Conflict-free Replicated Data Type)\npublic class GCounter {\n  private final Map<String, Long> counts = new ConcurrentHashMap<>();\n  private final String nodeId;\n\n  public GCounter(String nodeId) {\n    this.nodeId = nodeId;\n  }\n\n  public void increment() {\n    counts.merge(nodeId, 1L, Long::sum);\n  }\n\n  public long getValue() {\n    return counts.values().stream()\n      .mapToLong(Long::longValue)\n      .sum();\n  }\n\n  public void merge(GCounter other) {\n    other.counts.forEach((node, count) ->\n      counts.merge(node, count, Math::max)\n    );\n  }\n}\n\n// Saga Pattern for Distributed Transactions\n@Service\npublic class OrderSagaService {\n\n  @Autowired\n  private OrderService orderService;\n\n  @Autowired\n  private PaymentService paymentService;\n\n  @Autowired\n  private InventoryService inventoryService;\n\n  public void processOrder(OrderRequest request) {\n    String sagaId = UUID.randomUUID().toString();\n\n    try {\n      // Step 1: Create order\n      Order order = orderService.createOrder(request);\n\n      // Step 2: Reserve inventory\n      inventoryService.reserve(order.getItems(), sagaId);\n\n      // Step 3: Process payment\n      paymentService.charge(order.getTotalAmount(), sagaId);\n\n      // Success - commit all\n      orderService.confirmOrder(order.getId());\n\n    } catch (Exception e) {\n      // Compensating transactions\n      log.error(\"Saga failed, executing compensations\", e);\n\n      try {\n        paymentService.refund(sagaId);\n      } catch (Exception ex) {\n        log.error(\"Payment refund failed\", ex);\n      }\n\n      try {\n        inventoryService.release(sagaId);\n      } catch (Exception ex) {\n        log.error(\"Inventory release failed\", ex);\n      }\n\n      try {\n        orderService.cancelOrder(sagaId);\n      } catch (Exception ex) {\n        log.error(\"Order cancellation failed\", ex);\n      }\n\n      throw new SagaFailedException(\"Order processing failed\", e);\n    }\n  }\n}\n\n// BASE Properties Example\n@Service\npublic class BaseService {\n\n  // Basically Available - service responds even during partial failures\n  public List<Product> searchProducts(String query) {\n    try {\n      return primaryDatabase.search(query);\n    } catch (Exception e) {\n      log.warn(\"Primary search failed, using cache\", e);\n      return cacheService.search(query);  // Degraded but available\n    }\n  }\n\n  // Soft State - state may change without input (background sync)\n  @Scheduled(fixedRate = 60000)\n  public void syncState() {\n    List<Order> pendingOrders = orderRepository.findPendingOrders();\n\n    pendingOrders.forEach(order -> {\n      // Background state transitions\n      if (order.isPaid() && order.isShipped()) {\n        order.setStatus(OrderStatus.COMPLETED);\n        orderRepository.save(order);\n      }\n    });\n  }\n\n  // Eventual Consistency - accept temporary inconsistency\n  public void updateUserProfile(User user) {\n    // Update primary\n    userRepository.save(user);\n\n    // Async propagation to read replicas\n    CompletableFuture.runAsync(() -> {\n      replicaSync.sync(user);\n    });\n\n    // Async update cache\n    CompletableFuture.runAsync(() -> {\n      cacheService.update(user);\n    });\n  }\n}",
    "difficulty": "Medium"
  },
  {
    "id": 7,
    "title": "Disaster Recovery",
    "description": "Planning for system failures",
    "explanation": "Disaster Recovery (DR) ensures business continuity during catastrophic failures. Recovery Time Objective (RTO) defines maximum acceptable downtime. Recovery Point Objective (RPO) defines maximum acceptable data loss. Backup strategies include full, incremental, and differential backups. DR testing validates recovery procedures. Multi-region deployments provide geographic redundancy. Failover procedures automate recovery to backup systems.",
    "keyPoints": [
      "RTO (Recovery Time Objective) - maximum acceptable downtime duration",
      "RPO (Recovery Point Objective) - maximum acceptable data loss",
      "Backup strategies - full, incremental, differential backups with retention",
      "DR testing - regular drills to validate recovery procedures",
      "Multi-region - geographic redundancy for regional failures",
      "Failover procedures - automated recovery to backup infrastructure"
    ],
    "javaCode": "// Backup Service\n@Service\npublic class BackupService {\n\n  @Autowired\n  private DataSource dataSource;\n\n  @Scheduled(cron = \"0 0 2 * * ?\")  // Daily at 2 AM\n  public void performFullBackup() {\n    String timestamp = LocalDateTime.now()\n      .format(DateTimeFormatter.ofPattern(\"yyyyMMdd_HHmmss\"));\n    String backupFile = \"backup_full_\" + timestamp + \".sql\";\n\n    try {\n      ProcessBuilder pb = new ProcessBuilder(\n        \"mysqldump\",\n        \"--host=\" + dbHost,\n        \"--user=\" + dbUser,\n        \"--password=\" + dbPassword,\n        \"--databases\", dbName,\n        \"--result-file=\" + backupFile\n      );\n\n      Process process = pb.start();\n      int exitCode = process.waitFor();\n\n      if (exitCode == 0) {\n        // Upload to S3\n        uploadToS3(backupFile);\n        log.info(\"Full backup completed: {}\", backupFile);\n      } else {\n        log.error(\"Backup failed with exit code: {}\", exitCode);\n      }\n    } catch (Exception e) {\n      log.error(\"Backup error\", e);\n    }\n  }\n\n  @Scheduled(cron = \"0 0 */6 * * ?\")  // Every 6 hours\n  public void performIncrementalBackup() {\n    String timestamp = LocalDateTime.now()\n      .format(DateTimeFormatter.ofPattern(\"yyyyMMdd_HHmmss\"));\n\n    // Backup only changes since last backup\n    String backupFile = \"backup_incr_\" + timestamp + \".sql\";\n\n    try {\n      // Binary log-based incremental backup\n      mysqlBinlogBackup(backupFile);\n      uploadToS3(backupFile);\n      log.info(\"Incremental backup completed: {}\", backupFile);\n    } catch (Exception e) {\n      log.error(\"Incremental backup error\", e);\n    }\n  }\n\n  private void uploadToS3(String filename) {\n    AmazonS3 s3Client = AmazonS3ClientBuilder.defaultClient();\n    s3Client.putObject(\n      new PutObjectRequest(\n        \"backup-bucket\",\n        \"backups/\" + filename,\n        new File(filename)\n      )\n    );\n  }\n}\n\n// RTO/RPO Configuration\n@Configuration\npublic class DisasterRecoveryConfig {\n\n  // RTO: 4 hours - Maximum acceptable downtime\n  private final Duration RTO = Duration.ofHours(4);\n\n  // RPO: 1 hour - Maximum acceptable data loss\n  private final Duration RPO = Duration.ofHours(1);\n\n  @Bean\n  public BackupScheduler backupScheduler() {\n    return new BackupScheduler(RPO);\n  }\n\n  @Bean\n  public FailoverCoordinator failoverCoordinator() {\n    return new FailoverCoordinator(RTO);\n  }\n}\n\n// Multi-Region Failover\n@Service\npublic class MultiRegionService {\n\n  private final Map<String, RegionEndpoint> regions = new HashMap<>();\n  private volatile String primaryRegion = \"us-east-1\";\n\n  public MultiRegionService() {\n    regions.put(\"us-east-1\", new RegionEndpoint(\"https://api.us-east-1.example.com\"));\n    regions.put(\"us-west-2\", new RegionEndpoint(\"https://api.us-west-2.example.com\"));\n    regions.put(\"eu-west-1\", new RegionEndpoint(\"https://api.eu-west-1.example.com\"));\n\n    startHealthMonitoring();\n  }\n\n  public <T> T executeRequest(Function<RegionEndpoint, T> request) {\n    try {\n      return request.apply(regions.get(primaryRegion));\n    } catch (Exception e) {\n      log.error(\"Primary region failed, failing over\", e);\n      return executeWithFailover(request);\n    }\n  }\n\n  private <T> T executeWithFailover(Function<RegionEndpoint, T> request) {\n    for (Map.Entry<String, RegionEndpoint> entry : regions.entrySet()) {\n      if (entry.getKey().equals(primaryRegion)) continue;\n\n      try {\n        T result = request.apply(entry.getValue());\n        log.info(\"Failover successful to region: {}\", entry.getKey());\n        primaryRegion = entry.getKey();  // Update primary\n        return result;\n      } catch (Exception e) {\n        log.warn(\"Failover to {} failed\", entry.getKey(), e);\n      }\n    }\n\n    throw new AllRegionsFailedException();\n  }\n\n  private void startHealthMonitoring() {\n    ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);\n    scheduler.scheduleAtFixedRate(() -> {\n      for (Map.Entry<String, RegionEndpoint> entry : regions.entrySet()) {\n        boolean healthy = entry.getValue().healthCheck();\n        if (!healthy && entry.getKey().equals(primaryRegion)) {\n          log.error(\"Primary region {} is unhealthy, triggering failover\",\n            entry.getKey());\n          triggerFailover();\n        }\n      }\n    }, 0, 30, TimeUnit.SECONDS);\n  }\n\n  private void triggerFailover() {\n    // Find healthy region\n    regions.entrySet().stream()\n      .filter(e -> !e.getKey().equals(primaryRegion))\n      .filter(e -> e.getValue().healthCheck())\n      .findFirst()\n      .ifPresent(e -> {\n        log.info(\"Failing over from {} to {}\", primaryRegion, e.getKey());\n        primaryRegion = e.getKey();\n      });\n  }\n}\n\n// Restore Service\n@Service\npublic class RestoreService {\n\n  public void restoreFromBackup(String backupFile) {\n    log.info(\"Starting restore from backup: {}\", backupFile);\n\n    try {\n      // Download from S3\n      downloadFromS3(backupFile);\n\n      // Stop application (maintenance mode)\n      maintenanceModeService.enable();\n\n      // Restore database\n      ProcessBuilder pb = new ProcessBuilder(\n        \"mysql\",\n        \"--host=\" + dbHost,\n        \"--user=\" + dbUser,\n        \"--password=\" + dbPassword,\n        dbName\n      );\n\n      pb.redirectInput(new File(backupFile));\n      Process process = pb.start();\n      int exitCode = process.waitFor();\n\n      if (exitCode == 0) {\n        log.info(\"Database restored successfully\");\n\n        // Apply incremental backups if any\n        applyIncrementalBackups(backupFile);\n\n        // Restart application\n        maintenanceModeService.disable();\n      } else {\n        log.error(\"Restore failed with exit code: {}\", exitCode);\n      }\n    } catch (Exception e) {\n      log.error(\"Restore error\", e);\n    }\n  }\n\n  private void applyIncrementalBackups(String fullBackupFile) {\n    // Find incremental backups after full backup\n    LocalDateTime fullBackupTime = extractTimestamp(fullBackupFile);\n\n    List<String> incrementalBackups = findIncrementalBackups(fullBackupTime);\n\n    for (String incBackup : incrementalBackups) {\n      log.info(\"Applying incremental backup: {}\", incBackup);\n      applyBinaryLog(incBackup);\n    }\n  }\n}\n\n// DR Testing Framework\n@Service\npublic class DrTestService {\n\n  @Scheduled(cron = \"0 0 1 1 * ?\")  // Monthly DR drill\n  public void performDrTest() {\n    log.info(\"Starting DR test drill\");\n\n    DrTestReport report = new DrTestReport();\n    report.setStartTime(LocalDateTime.now());\n\n    try {\n      // Test 1: Database backup and restore\n      report.addTest(testBackupRestore());\n\n      // Test 2: Failover to secondary region\n      report.addTest(testRegionalFailover());\n\n      // Test 3: Application recovery\n      report.addTest(testApplicationRecovery());\n\n      // Test 4: Data consistency check\n      report.addTest(testDataConsistency());\n\n      report.setStatus(\"SUCCESS\");\n    } catch (Exception e) {\n      report.setStatus(\"FAILED\");\n      report.setError(e.getMessage());\n      log.error(\"DR test failed\", e);\n    }\n\n    report.setEndTime(LocalDateTime.now());\n    report.setDuration(Duration.between(report.getStartTime(), report.getEndTime()));\n\n    // Send report\n    sendDrTestReport(report);\n  }\n\n  private TestResult testBackupRestore() {\n    // Create test backup\n    backupService.performFullBackup();\n\n    // Restore to test environment\n    restoreService.restoreToTestEnvironment();\n\n    // Verify data\n    boolean dataValid = verifyTestData();\n\n    return new TestResult(\"Backup/Restore\", dataValid);\n  }\n\n  private TestResult testRegionalFailover() {\n    // Simulate primary region failure\n    multiRegionService.simulateRegionFailure(\"us-east-1\");\n\n    // Execute request (should failover)\n    boolean success = multiRegionService.executeHealthCheck();\n\n    return new TestResult(\"Regional Failover\", success);\n  }\n}\n\n// Automated Failover Coordinator\n@Component\npublic class FailoverCoordinator {\n\n  private final Duration rto;\n  private volatile boolean failoverInProgress = false;\n\n  public void initiateFailover(FailureEvent event) {\n    if (failoverInProgress) {\n      log.warn(\"Failover already in progress\");\n      return;\n    }\n\n    failoverInProgress = true;\n    long startTime = System.currentTimeMillis();\n\n    try {\n      log.info(\"Initiating failover due to: {}\", event.getReason());\n\n      // Step 1: Stop traffic to failed component\n      loadBalancer.removeBackend(event.getFailedComponent());\n\n      // Step 2: Promote standby to primary\n      standbyService.promote(event.getStandbyComponent());\n\n      // Step 3: Redirect traffic\n      loadBalancer.addBackend(event.getStandbyComponent());\n\n      // Step 4: Verify failover\n      if (!verifyFailover()) {\n        rollbackFailover();\n        throw new FailoverException(\"Failover verification failed\");\n      }\n\n      long duration = System.currentTimeMillis() - startTime;\n      log.info(\"Failover completed in {}ms\", duration);\n\n      // Check if within RTO\n      if (duration > rto.toMillis()) {\n        log.error(\"Failover exceeded RTO: {}ms > {}ms\", duration, rto.toMillis());\n      }\n\n    } finally {\n      failoverInProgress = false;\n    }\n  }\n}",
    "difficulty": "Medium"
  },
  {
    "id": 8,
    "title": "Performance Optimization",
    "description": "System performance tuning",
    "explanation": "Performance optimization improves system responsiveness and throughput. Database indexing accelerates query execution on frequently searched columns. Connection pooling reuses database connections to reduce overhead. Asynchronous processing offloads long-running tasks. Profiling tools identify performance bottlenecks. Query optimization includes proper indexing, avoiding N+1 queries, and using appropriate fetch strategies.",
    "keyPoints": [
      "Database indexing - B-tree indexes on WHERE/JOIN columns, composite indexes",
      "Connection pooling - HikariCP with tuned pool size, connection reuse",
      "Async processing - @Async, CompletableFuture, message queues for background tasks",
      "Profiling - JProfiler, VisualVM, Spring Boot Actuator metrics",
      "Query optimization - EXPLAIN plans, avoid SELECT *, proper JOIN strategies",
      "JPA optimization - fetch strategies (LAZY/EAGER), batch inserts, query hints"
    ],
    "javaCode": "// Database Indexing\n/*\n-- Create indexes on frequently queried columns\nCREATE INDEX idx_users_email ON users(email);\nCREATE INDEX idx_orders_user_id ON orders(user_id);\nCREATE INDEX idx_orders_status ON orders(status);\n\n-- Composite index for multiple columns\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n\n-- Covering index includes all query columns\nCREATE INDEX idx_orders_covering ON orders(user_id, created_at, status, total_amount);\n\n-- Check index usage\nEXPLAIN SELECT * FROM orders WHERE user_id = 123 AND status = 'PENDING';\n*/\n\n// Connection Pooling - HikariCP\n@Configuration\npublic class DataSourceConfig {\n\n  @Bean\n  public DataSource dataSource() {\n    HikariConfig config = new HikariConfig();\n    config.setJdbcUrl(\"jdbc:mysql://localhost:3306/mydb\");\n    config.setUsername(\"user\");\n    config.setPassword(\"password\");\n\n    // Performance tuning\n    config.setMaximumPoolSize(20);           // Max connections\n    config.setMinimumIdle(5);                // Min idle connections\n    config.setConnectionTimeout(30000);      // 30 seconds\n    config.setIdleTimeout(600000);           // 10 minutes\n    config.setMaxLifetime(1800000);          // 30 minutes\n\n    // Performance optimizations\n    config.setAutoCommit(false);\n    config.setCachePrepStmts(true);\n    config.setPrepStmtCacheSize(250);\n    config.setPrepStmtCacheSqlLimit(2048);\n    config.setUseServerPrepStmts(true);\n\n    return new HikariDataSource(config);\n  }\n}\n\n// Async Processing\n@Configuration\n@EnableAsync\npublic class AsyncConfig {\n\n  @Bean\n  public Executor taskExecutor() {\n    ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();\n    executor.setCorePoolSize(10);\n    executor.setMaxPoolSize(50);\n    executor.setQueueCapacity(200);\n    executor.setThreadNamePrefix(\"async-\");\n    executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());\n    executor.initialize();\n    return executor;\n  }\n}\n\n@Service\npublic class EmailService {\n\n  @Async\n  public CompletableFuture<Void> sendEmailAsync(String to, String subject, String body) {\n    log.info(\"Sending email asynchronously to: {}\", to);\n\n    try {\n      // Simulate email sending\n      Thread.sleep(2000);\n      log.info(\"Email sent to: {}\", to);\n      return CompletableFuture.completedFuture(null);\n    } catch (InterruptedException e) {\n      return CompletableFuture.failedFuture(e);\n    }\n  }\n}\n\n@Service\npublic class OrderService {\n\n  @Autowired\n  private EmailService emailService;\n\n  public Order createOrder(OrderRequest request) {\n    Order order = orderRepository.save(new Order(request));\n\n    // Send confirmation email asynchronously (non-blocking)\n    emailService.sendEmailAsync(\n      order.getUserEmail(),\n      \"Order Confirmation\",\n      \"Your order \" + order.getId() + \" has been placed\"\n    );\n\n    return order;  // Return immediately\n  }\n}\n\n// Query Optimization - Avoid N+1\n// BAD: N+1 Query Problem\n@Service\npublic class BadQueryService {\n\n  public List<OrderDto> getOrders() {\n    List<Order> orders = orderRepository.findAll();  // 1 query\n\n    return orders.stream()\n      .map(order -> {\n        // N queries (one per order)\n        User user = userRepository.findById(order.getUserId()).orElseThrow();\n        return new OrderDto(order, user);\n      })\n      .collect(Collectors.toList());\n  }\n}\n\n// GOOD: Fetch Join\n@Repository\npublic interface OrderRepository extends JpaRepository<Order, Long> {\n\n  @Query(\"SELECT o FROM Order o JOIN FETCH o.user\")\n  List<Order> findAllWithUsers();  // Single query with JOIN\n\n  @Query(\"SELECT o FROM Order o LEFT JOIN FETCH o.items WHERE o.id = :id\")\n  Optional<Order> findByIdWithItems(@Param(\"id\") Long id);\n}\n\n@Service\npublic class GoodQueryService {\n\n  public List<OrderDto> getOrders() {\n    List<Order> orders = orderRepository.findAllWithUsers();  // 1 query\n    return orders.stream()\n      .map(OrderDto::new)\n      .collect(Collectors.toList());\n  }\n}\n\n// JPA Performance - Batch Inserts\n@Configuration\npublic class JpaConfig {\n\n  @Bean\n  public LocalContainerEntityManagerFactoryBean entityManagerFactory() {\n    LocalContainerEntityManagerFactoryBean em = new LocalContainerEntityManagerFactoryBean();\n\n    Map<String, Object> properties = new HashMap<>();\n    properties.put(\"hibernate.jdbc.batch_size\", 50);\n    properties.put(\"hibernate.order_inserts\", true);\n    properties.put(\"hibernate.order_updates\", true);\n    properties.put(\"hibernate.jdbc.batch_versioned_data\", true);\n\n    em.setJpaPropertyMap(properties);\n    return em;\n  }\n}\n\n@Service\npublic class BatchInsertService {\n\n  @Autowired\n  private EntityManager entityManager;\n\n  @Transactional\n  public void batchInsert(List<User> users) {\n    int batchSize = 50;\n\n    for (int i = 0; i < users.size(); i++) {\n      entityManager.persist(users.get(i));\n\n      if (i % batchSize == 0 && i > 0) {\n        entityManager.flush();\n        entityManager.clear();\n      }\n    }\n\n    entityManager.flush();\n    entityManager.clear();\n  }\n}\n\n// Lazy vs Eager Fetching\n@Entity\npublic class Order {\n\n  @Id\n  private Long id;\n\n  // LAZY: Load only when accessed\n  @OneToMany(fetch = FetchType.LAZY, mappedBy = \"order\")\n  private List<OrderItem> items;\n\n  // EAGER: Load immediately (use sparingly)\n  @ManyToOne(fetch = FetchType.EAGER)\n  @JoinColumn(name = \"user_id\")\n  private User user;\n}\n\n// Profiling and Monitoring\n@Configuration\npublic class MetricsConfig {\n\n  @Bean\n  public MeterRegistryCustomizer<MeterRegistry> metricsCommonTags() {\n    return registry -> registry.config()\n      .commonTags(\"application\", \"order-service\");\n  }\n}\n\n@Service\npublic class ProfiledService {\n\n  @Timed(value = \"order.creation\", description = \"Time to create order\")\n  public Order createOrder(OrderRequest request) {\n    return orderRepository.save(new Order(request));\n  }\n\n  @Counted(value = \"order.failures\", description = \"Failed order creations\")\n  public void handleOrderFailure(Exception e) {\n    log.error(\"Order creation failed\", e);\n  }\n}\n\n// Caching for Performance\n@Service\npublic class CachedProductService {\n\n  @Cacheable(value = \"products\", key = \"#id\")\n  public Product getProduct(Long id) {\n    log.info(\"Loading product from database: {}\", id);\n    return productRepository.findById(id).orElseThrow();\n  }\n\n  @Cacheable(value = \"popularProducts\")\n  public List<Product> getPopularProducts() {\n    log.info(\"Loading popular products from database\");\n    return productRepository.findTop10ByOrderBySalesDesc();\n  }\n}\n\n// Query Hints\n@Repository\npublic interface UserRepository extends JpaRepository<User, Long> {\n\n  @QueryHints({\n    @QueryHint(name = \"org.hibernate.cacheable\", value = \"true\"),\n    @QueryHint(name = \"org.hibernate.fetchSize\", value = \"50\")\n  })\n  @Query(\"SELECT u FROM User u WHERE u.active = true\")\n  List<User> findActiveUsers();\n}\n\n// Database Query Analysis\n/*\n-- Slow query log analysis\nSHOW VARIABLES LIKE 'slow_query_log%';\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 2;\n\n-- Analyze query performance\nEXPLAIN ANALYZE\nSELECT o.*, u.name\nFROM orders o\nJOIN users u ON o.user_id = u.id\nWHERE o.status = 'PENDING'\n  AND o.created_at > DATE_SUB(NOW(), INTERVAL 7 DAY);\n\n-- Index usage\nSHOW INDEX FROM orders;\nANALYZE TABLE orders;\n*/\n\n// Pagination for Large Result Sets\n@Service\npublic class PaginatedService {\n\n  public Page<Order> getOrders(int page, int size) {\n    Pageable pageable = PageRequest.of(page, size,\n      Sort.by(\"createdAt\").descending());\n\n    return orderRepository.findAll(pageable);\n  }\n\n  // Cursor-based pagination for consistent results\n  public List<Order> getOrdersAfter(Long lastId, int limit) {\n    return orderRepository.findTop10ByIdGreaterThanOrderById(lastId);\n  }\n}",
    "difficulty": "Medium"
  },
  {
    "id": 9,
    "title": "System Design Calculations",
    "description": "Capacity planning and estimation formulas",
    "explanation": "System design calculations are essential for capacity planning, resource estimation, and architectural decisions. Key calculations include storage requirements based on user growth and data retention, bandwidth estimation using request rates and payload sizes, throughput calculations for queries per second, and memory sizing for caching strategies. Understanding these calculations helps in making informed decisions about infrastructure, costs, and scalability requirements during system design interviews and real-world implementations.",
    "keyPoints": [
      "Storage calculations - data per user \u00d7 users \u00d7 retention period, including replication factor",
      "Bandwidth estimation - requests/sec \u00d7 average request size \u00d7 8 (bits) = Mbps",
      "QPS calculations - daily active users \u00d7 actions/day \u00f7 86400 sec, peak = 2-3\u00d7 average",
      "Memory sizing - cache hit ratio target, working set size, Redis/Memcached capacity",
      "Database sizing - rows \u00d7 row size, index overhead (~15-20%), growth buffer",
      "CDN calculations - total bandwidth \u00d7 CDN coverage percentage \u00d7 cost per GB"
    ],
    "javaCode": "// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n// \u2726 STORAGE CALCULATIONS\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n/*\nExample: Twitter-like Social Media Platform\n\nAssumptions:\n- 500 million users\n- 10% are daily active users (DAU) = 50 million\n- Each user posts 2 tweets per day on average\n- Each tweet: 140 characters (280 bytes) + metadata (150 bytes) = 430 bytes\n- Average 1 photo per 5 tweets, each photo = 200 KB\n- Data retention: 5 years\n- Replication factor: 3 (primary + 2 replicas)\n\nDaily Tweet Storage:\n50M DAU \u00d7 2 tweets = 100M tweets/day\n100M tweets \u00d7 430 bytes = 43 GB/day (text)\n100M tweets \u00f7 5 \u00d7 200 KB = 4 TB/day (photos)\nTotal daily: ~4.043 TB/day\n\nAnnual Storage:\n4.043 TB/day \u00d7 365 days = 1,476 TB/year = 1.48 PB/year\n\n5-Year Storage (with replication):\n1.48 PB/year \u00d7 5 years \u00d7 3 replicas = 22.2 PB total\n\nGrowth Buffer (20%):\n22.2 PB \u00d7 1.2 = 26.6 PB required storage\n*/\n\n@Service\npublic class StorageCalculator {\n\n  public StorageEstimate calculateStorage(StorageParams params) {\n    // Daily storage\n    long tweetsPerDay = params.getDailyActiveUsers() * params.getTweetsPerUser();\n    long textStoragePerDay = tweetsPerDay * params.getAvgTweetSize();\n    long photoStoragePerDay = (tweetsPerDay / params.getPhotoFrequency())\n                               * params.getAvgPhotoSize();\n    long totalDailyStorage = textStoragePerDay + photoStoragePerDay;\n\n    // Annual and multi-year storage\n    long annualStorage = totalDailyStorage * 365;\n    long multiYearStorage = annualStorage * params.getRetentionYears();\n\n    // With replication and buffer\n    long withReplication = multiYearStorage * params.getReplicationFactor();\n    long finalStorage = (long) (withReplication * params.getGrowthBuffer());\n\n    return new StorageEstimate(\n      toReadableSize(totalDailyStorage),\n      toReadableSize(annualStorage),\n      toReadableSize(finalStorage)\n    );\n  }\n}\n\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n// \u2726 BANDWIDTH CALCULATIONS\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n/*\nExample: Video Streaming Platform\n\nAssumptions:\n- 100 million daily active users\n- Average 2 hours of video watched per user per day\n- Video bitrates: 720p = 2.5 Mbps, 1080p = 5 Mbps, 4K = 25 Mbps\n- Distribution: 50% 720p, 40% 1080p, 10% 4K\n- Peak traffic is 3\u00d7 average\n\nAverage Bandwidth:\n720p: 50M users \u00d7 0.5 \u00d7 2.5 Mbps = 62.5 Tbps\n1080p: 50M users \u00d7 0.4 \u00d7 5 Mbps = 100 Tbps\n4K: 50M users \u00d7 0.1 \u00d7 25 Mbps = 125 Tbps\nTotal average: 287.5 Tbps\n\nPeak Bandwidth:\n287.5 Tbps \u00d7 3 = 862.5 Tbps peak\n\nDaily Data Transfer:\n287.5 Tbps \u00d7 86400 seconds = 24.8 exabytes/day\n*/\n\n@Service\npublic class BandwidthCalculator {\n\n  public BandwidthEstimate calculateBandwidth(VideoStreamingParams params) {\n    long activeUsers = params.getDailyActiveUsers();\n\n    // Calculate concurrent users during peak\n    // Assuming peak hour has 20% of daily users watching simultaneously\n    long peakConcurrentUsers = (long) (activeUsers * 0.2);\n\n    // Calculate bandwidth by quality\n    double bandwidth720p = peakConcurrentUsers * 0.5 * 2.5; // Mbps\n    double bandwidth1080p = peakConcurrentUsers * 0.4 * 5.0; // Mbps\n    double bandwidth4K = peakConcurrentUsers * 0.1 * 25.0; // Mbps\n\n    double totalPeakBandwidth = bandwidth720p + bandwidth1080p + bandwidth4K;\n    double averageBandwidth = totalPeakBandwidth / 3; // Peak is 3\u00d7 average\n\n    // Daily data transfer (in TB)\n    double dailyDataTransfer = (averageBandwidth * 86400) / 8 / 1024 / 1024; // TB\n\n    return new BandwidthEstimate(\n      averageBandwidth + \" Mbps average\",\n      totalPeakBandwidth + \" Mbps peak\",\n      String.format(\"%.2f TB/day\", dailyDataTransfer)\n    );\n  }\n}\n\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n// \u2726 QPS (QUERIES PER SECOND) CALCULATIONS\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n/*\nExample: E-commerce Platform API\n\nAssumptions:\n- 10 million daily active users\n- Each user makes 50 requests per day (browsing, searching, cart, checkout)\n- Peak traffic: 3\u00d7 average\n- Read:Write ratio = 100:1\n\nAverage QPS:\nTotal requests/day = 10M users \u00d7 50 requests = 500M requests/day\nAverage QPS = 500M \u00f7 86400 seconds = 5,787 QPS\n\nPeak QPS:\nPeak QPS = 5,787 \u00d7 3 = 17,361 QPS\n\nRead vs Write:\nRead QPS = 17,361 \u00d7 (100/101) = 17,189 reads/sec\nWrite QPS = 17,361 \u00d7 (1/101) = 172 writes/sec\n\nDatabase Connections Needed:\nAssuming each request takes 50ms average:\nConnections = QPS \u00d7 average_latency\nConnections = 17,361 \u00d7 0.05 = 868 connections\nWith 30% buffer = 1,128 connections\n*/\n\n@Service\npublic class QPSCalculator {\n\n  public QPSEstimate calculateQPS(ApiTrafficParams params) {\n    long dailyActiveUsers = params.getDailyActiveUsers();\n    int requestsPerUser = params.getRequestsPerUser();\n    double peakMultiplier = params.getPeakMultiplier();\n\n    // Calculate average and peak QPS\n    long totalDailyRequests = dailyActiveUsers * requestsPerUser;\n    double averageQPS = totalDailyRequests / 86400.0;\n    double peakQPS = averageQPS * peakMultiplier;\n\n    // Read/Write split\n    double readWriteRatio = params.getReadWriteRatio();\n    double readQPS = peakQPS * (readWriteRatio / (readWriteRatio + 1));\n    double writeQPS = peakQPS * (1 / (readWriteRatio + 1));\n\n    // Database connections needed\n    double avgLatency = params.getAvgLatencySeconds();\n    int connectionsNeeded = (int) Math.ceil(peakQPS * avgLatency * 1.3); // 30% buffer\n\n    return new QPSEstimate(\n      String.format(\"%.0f QPS average\", averageQPS),\n      String.format(\"%.0f QPS peak\", peakQPS),\n      String.format(\"%.0f reads/sec, %.0f writes/sec\", readQPS, writeQPS),\n      connectionsNeeded + \" DB connections recommended\"\n    );\n  }\n}\n\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n// \u2726 CACHE MEMORY CALCULATIONS\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n/*\nExample: Redis Cache for User Sessions and Hot Data\n\nAssumptions:\n- 50 million daily active users\n- 10% concurrent users at peak = 5 million concurrent\n- Session data per user: 10 KB\n- Hot products cache: 100K products \u00d7 5 KB = 500 MB\n- Hot user profiles: 1M users \u00d7 2 KB = 2 GB\n- Cache hit ratio target: 95%\n- Overhead and fragmentation: 20%\n\nSession Cache:\n5M concurrent users \u00d7 10 KB = 50 GB\n\nHot Data Cache:\nProducts: 500 MB\nUser profiles: 2 GB\nFeed cache: 10 GB (estimated)\nTotal hot data: 12.5 GB\n\nTotal Cache Memory:\n(50 GB + 12.5 GB) \u00d7 1.2 (overhead) = 75 GB required\n\nRecommendation: 100 GB Redis cluster (with headroom for growth)\n*/\n\n@Service\npublic class CacheCalculator {\n\n  public CacheEstimate calculateCacheSize(CacheParams params) {\n    long concurrentUsers = params.getConcurrentUsers();\n    int sessionSizeKB = params.getSessionSizeKB();\n\n    // Session cache\n    long sessionCacheMB = (concurrentUsers * sessionSizeKB) / 1024;\n\n    // Hot data cache\n    long productCacheMB = (params.getHotProducts() * params.getProductSizeKB()) / 1024;\n    long userCacheMB = (params.getHotUsers() * params.getUserProfileSizeKB()) / 1024;\n    long miscCacheMB = params.getMiscCacheMB();\n\n    long totalHotDataMB = productCacheMB + userCacheMB + miscCacheMB;\n\n    // Total with overhead\n    long totalCacheMB = (long) ((sessionCacheMB + totalHotDataMB) * 1.2);\n\n    // Recommend next tier size\n    long recommendedGB = ((totalCacheMB / 1024) / 32 + 1) * 32; // Round up to 32 GB increments\n\n    return new CacheEstimate(\n      sessionCacheMB + \" MB session cache\",\n      totalHotDataMB + \" MB hot data cache\",\n      totalCacheMB + \" MB total required\",\n      recommendedGB + \" GB recommended (with growth buffer)\"\n    );\n  }\n}\n\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n// \u2726 DATABASE SHARDING CALCULATIONS\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n/*\nExample: User Database Sharding Strategy\n\nAssumptions:\n- 500 million total users\n- Each user record: 2 KB (base data) + 5 KB (extended profile) = 7 KB\n- Index overhead: 20%\n- Maximum shard size: 500 GB (for backup/recovery time)\n- Growth rate: 20% per year\n\nTotal Database Size:\n500M users \u00d7 7 KB = 3.5 TB (data only)\nWith indexes: 3.5 TB \u00d7 1.2 = 4.2 TB\n\nNumber of Shards:\n4.2 TB \u00f7 0.5 TB per shard = 8.4 shards\nRound up to 16 shards (for even distribution and growth)\n\nSharding Key: user_id % 16 (hash-based sharding)\n\nUsers per shard: 500M \u00f7 16 = 31.25M users/shard\nData per shard: 4.2 TB \u00f7 16 = 262 GB/shard (with room to grow to 500 GB)\n\n1-Year Growth:\nUsers: 500M \u00d7 1.2 = 600M users\nData: 4.2 TB \u00d7 1.2 = 5.04 TB\nPer shard: 5.04 TB \u00f7 16 = 315 GB/shard (still under 500 GB limit)\n*/\n\n@Service\npublic class ShardingCalculator {\n\n  public ShardingEstimate calculateSharding(DatabaseParams params) {\n    long totalUsers = params.getTotalUsers();\n    int recordSizeKB = params.getRecordSizeKB();\n    double indexOverhead = params.getIndexOverhead();\n    int maxShardGB = params.getMaxShardSizeGB();\n\n    // Calculate total database size\n    long totalDataGB = (totalUsers * recordSizeKB) / 1024 / 1024;\n    long totalWithIndexesGB = (long) (totalDataGB * (1 + indexOverhead));\n\n    // Calculate number of shards\n    int minShards = (int) Math.ceil((double) totalWithIndexesGB / maxShardGB);\n    // Round up to nearest power of 2 for even distribution\n    int recommendedShards = (int) Math.pow(2, Math.ceil(Math.log(minShards) / Math.log(2)));\n\n    long usersPerShard = totalUsers / recommendedShards;\n    long dataPerShardGB = totalWithIndexesGB / recommendedShards;\n\n    // Calculate growth headroom\n    double growthRate = params.getAnnualGrowthRate();\n    long futureDataPerShardGB = (long) (dataPerShardGB * (1 + growthRate));\n    int yearsUntilReshard = 0;\n    while (futureDataPerShardGB < maxShardGB && yearsUntilReshard < 5) {\n      yearsUntilReshard++;\n      futureDataPerShardGB = (long) (dataPerShardGB * Math.pow(1 + growthRate, yearsUntilReshard));\n    }\n\n    return new ShardingEstimate(\n      totalWithIndexesGB + \" GB total database size\",\n      recommendedShards + \" shards recommended\",\n      usersPerShard + \" users per shard\",\n      dataPerShardGB + \" GB per shard currently\",\n      yearsUntilReshard + \" years until resharding needed\"\n    );\n  }\n}\n\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n// \u2726 CDN COST CALCULATIONS\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n/*\nExample: CDN for Static Assets and Media\n\nAssumptions:\n- 100 million page views per day\n- Average page size: 2 MB (HTML, CSS, JS, images)\n- 80% of content served via CDN\n- CDN cost: $0.08 per GB\n- Cache hit ratio: 90%\n\nDaily Data Transfer:\nTotal: 100M views \u00d7 2 MB = 200 TB/day\nVia CDN: 200 TB \u00d7 0.8 = 160 TB/day\nOrigin traffic (cache misses): 160 TB \u00d7 0.1 = 16 TB/day\n\nMonthly CDN Costs:\nCDN data: 160 TB/day \u00d7 30 days = 4,800 TB/month\nCost: 4,800 TB \u00d7 $0.08/GB = 4,800,000 GB \u00d7 $0.08 = $384,000/month\n\nOrigin Bandwidth:\n16 TB/day \u00d7 30 days = 480 TB/month origin traffic\n*/\n\n@Service\npublic class CDNCalculator {\n\n  public CDNCostEstimate calculateCDNCost(CDNParams params) {\n    long dailyPageViews = params.getDailyPageViews();\n    double avgPageSizeMB = params.getAvgPageSizeMB();\n    double cdnCoveragePercent = params.getCdnCoveragePercent();\n    double cacheHitRatio = params.getCacheHitRatio();\n    double costPerGB = params.getCostPerGB();\n\n    // Calculate daily data transfer\n    double totalDailyTB = (dailyPageViews * avgPageSizeMB) / 1024 / 1024;\n    double cdnDailyTB = totalDailyTB * cdnCoveragePercent;\n    double originDailyTB = cdnDailyTB * (1 - cacheHitRatio);\n\n    // Calculate monthly costs\n    double cdnMonthlyTB = cdnDailyTB * 30;\n    double cdnMonthlyGB = cdnMonthlyTB * 1024;\n    double monthlyCost = cdnMonthlyGB * costPerGB;\n\n    double originMonthlyTB = originDailyTB * 30;\n\n    return new CDNCostEstimate(\n      String.format(\"%.2f TB/day via CDN\", cdnDailyTB),\n      String.format(\"%.2f TB/day from origin\", originDailyTB),\n      String.format(\"%.2f TB/month CDN transfer\", cdnMonthlyTB),\n      String.format(\"$%.2f/month CDN cost\", monthlyCost),\n      String.format(\"%.1f%% cache hit ratio saves $%.2f/month\",\n                    cacheHitRatio * 100,\n                    originMonthlyTB * 1024 * costPerGB)\n    );\n  }\n}\n\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n// \u2726 LATENCY BUDGET CALCULATIONS\n// \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n/*\nExample: API Request Latency Budget (Target: 200ms p99)\n\nLatency Budget Breakdown:\n1. Load Balancer: 5ms\n2. API Gateway: 10ms\n3. Authentication Service: 15ms\n4. Business Logic: 50ms\n5. Database Query: 80ms (primary bottleneck)\n6. External API Call: 30ms\n7. Response Serialization: 5ms\n8. Network Transfer: 5ms\n\nTotal: 200ms (at p99)\n\nOptimization Strategies:\n- Cache authentication tokens (reduce from 15ms to 2ms) = 13ms saved\n- Add database read replica (reduce from 80ms to 30ms) = 50ms saved\n- Cache frequent queries (hit ratio 70%, reduce to 5ms) = 52.5ms saved average\n- Async external API (parallel, not serial) = 30ms saved\n\nOptimized Latency: ~95ms p99 (after optimizations)\n*/\n\n@Service\npublic class LatencyCalculator {\n\n  public LatencyBreakdown calculateLatencyBudget(LatencyParams params) {\n    Map<String, Double> components = params.getLatencyComponents();\n    double totalLatency = components.values().stream()\n                                    .mapToDouble(Double::doubleValue)\n                                    .sum();\n\n    // Find bottlenecks (components > 25% of total)\n    List<String> bottlenecks = components.entrySet().stream()\n      .filter(e -> e.getValue() / totalLatency > 0.25)\n      .map(Map.Entry::getKey)\n      .collect(Collectors.toList());\n\n    // Calculate optimization impact\n    Map<String, Double> optimizations = calculateOptimizations(components);\n    double optimizedLatency = totalLatency - optimizations.values().stream()\n                                                          .mapToDouble(Double::doubleValue)\n                                                          .sum();\n\n    return new LatencyBreakdown(\n      totalLatency + \" ms current p99 latency\",\n      bottlenecks.toString() + \" are bottlenecks\",\n      optimizedLatency + \" ms after optimizations\",\n      String.format(\"%.1f%% latency reduction possible\",\n                    ((totalLatency - optimizedLatency) / totalLatency) * 100)\n    );\n  }\n\n  private Map<String, Double> calculateOptimizations(Map<String, Double> components) {\n    Map<String, Double> optimizations = new HashMap<>();\n\n    // Authentication caching\n    if (components.containsKey(\"authentication\")) {\n      optimizations.put(\"cache_auth\", components.get(\"authentication\") * 0.85);\n    }\n\n    // Database read replicas\n    if (components.containsKey(\"database\")) {\n      optimizations.put(\"read_replica\", components.get(\"database\") * 0.6);\n    }\n\n    // Query caching (70% hit ratio)\n    if (components.containsKey(\"database\")) {\n      optimizations.put(\"query_cache\", components.get(\"database\") * 0.7 * 0.95);\n    }\n\n    return optimizations;\n  }\n}\n\n/*\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nQUICK REFERENCE FORMULAS\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nStorage:\n- Daily Storage = Users \u00d7 Actions/Day \u00d7 Data/Action\n- Total Storage = Daily Storage \u00d7 Retention Days \u00d7 Replication Factor \u00d7 Growth Buffer\n\nBandwidth:\n- Bandwidth (Mbps) = Requests/Second \u00d7 Average Size (bytes) \u00d7 8 bits/byte / 1,000,000\n- Peak Bandwidth = Average Bandwidth \u00d7 Peak Multiplier (typically 2-3\u00d7)\n\nQPS:\n- Average QPS = Total Daily Requests / 86,400 seconds\n- Peak QPS = Average QPS \u00d7 Peak Multiplier\n- DB Connections = Peak QPS \u00d7 Average Latency (seconds) \u00d7 Buffer (1.3)\n\nMemory:\n- Cache Size = (Hot Data Set + Session Data) \u00d7 Overhead Factor (1.2-1.5)\n- Cache Hit Ratio = Cached Requests / Total Requests (target: 80-95%)\n\nDatabase:\n- Table Size = Rows \u00d7 Row Size \u00d7 (1 + Index Overhead)\n- Shards Needed = Total Size / Max Shard Size (round up to power of 2)\n\nCDN:\n- Monthly Cost = Daily Data Transfer (GB) \u00d7 30 \u00d7 Cost/GB\n- Origin Traffic = CDN Traffic \u00d7 (1 - Cache Hit Ratio)\n\nThroughput:\n- Throughput = Concurrency / Latency\n- Little's Law: L = \u03bb \u00d7 W (requests in system = arrival rate \u00d7 wait time)\n*/",
    "difficulty": "Medium"
  }
]